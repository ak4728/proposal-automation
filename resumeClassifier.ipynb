{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdullah.kurkcu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import json\n",
    "from re import findall\n",
    "from re import sub\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import PyPDF2\n",
    "import glob\n",
    "import en_core_web_sm\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import re  \n",
    "import nltk  \n",
    "from sklearn.datasets import load_files \n",
    "nltk.download('stopwords')  \n",
    "import pickle  \n",
    "from nltk.corpus import stopwords\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../resumesnot\\19.00150_Town of Vail_Gore Valley Trail_Feb19_FINAL_LR.pdf\n",
      "../resumesnot\\21.00876 Becker County Heartland Trail Engineering & Design Services - Final.pdf\n",
      "../resumesnot\\21.00980 Sioux City Proposal.pdf\n"
     ]
    }
   ],
   "source": [
    "# All the pdf files in the folder\n",
    "pdf_files = glob.glob(\"%s/*.pdf\" % \"../resumesnot/\")\n",
    "\n",
    "\n",
    "# Resumes -> convert to a dictionary with an employee name\n",
    "resumesTest = pd.DataFrame()\n",
    "\n",
    "texts = []\n",
    "# File loop\n",
    "for file in pdf_files:\n",
    "    print(file)\n",
    "    text = \"\"\n",
    "    try:\n",
    "        fileReader = PyPDF2.PdfFileReader(open(file,'rb'))\n",
    "        count = fileReader.numPages\n",
    "        text = \"\"\n",
    "        i = 0\n",
    "        while i < count:\n",
    "            text += fileReader.getPage(i).extractText()\n",
    "            i = i +1  \n",
    "        # lowercase\n",
    "        text = text.lower()\n",
    "        # remove nubmers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        # remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # remove \\n\n",
    "        text = text.replace('\\n',' ')\n",
    "        # remove ndł\n",
    "        text= text.replace('ndł',' ')\n",
    "        # remove \\nł\n",
    "        text = text.replace('\\nł',' ')\n",
    "        # remove ł\n",
    "        text = text.replace('ł',' ')\n",
    "        # remove \\nł\n",
    "        text = text.replace('\\nł',' ')\n",
    "\n",
    "        texts.append(text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    \n",
    "resumesTest['text'] = texts\n",
    "resumesTest['classification'] = \"NotResume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../resumes\\17.00532 0.pdf\n",
      "../resumes\\17.00532 1.pdf\n",
      "../resumes\\17.00532 2.pdf\n",
      "../resumes\\17.00532 3.pdf\n",
      "../resumes\\18.00308 0.pdf\n",
      "../resumes\\18.00308 1.pdf\n",
      "../resumes\\18.00308 2.pdf\n",
      "../resumes\\18.00308 3.pdf\n",
      "../resumes\\19.00064 0.pdf\n",
      "../resumes\\19.00064 1.pdf\n",
      "../resumes\\19.00064 10.pdf\n",
      "../resumes\\19.00064 11.pdf\n",
      "../resumes\\19.00064 2.pdf\n",
      "../resumes\\19.00064 3.pdf\n",
      "../resumes\\19.00064 4.pdf\n",
      "../resumes\\19.00064 5.pdf\n",
      "../resumes\\19.00064 6.pdf\n",
      "../resumes\\19.00064 7.pdf\n",
      "../resumes\\19.00064 8.pdf\n",
      "../resumes\\19.00064 9.pdf\n",
      "../resumes\\20.00432 0.pdf\n",
      "../resumes\\20.00432 1.pdf\n",
      "../resumes\\20.00432 2.pdf\n",
      "../resumes\\20.00432 3.pdf\n",
      "../resumes\\20.00432 4.pdf\n",
      "../resumes\\20.00973 0.pdf\n",
      "../resumes\\20.00973 1.pdf\n",
      "../resumes\\20.00973 10.pdf\n",
      "../resumes\\20.00973 11.pdf\n",
      "../resumes\\20.00973 12.pdf\n",
      "../resumes\\20.00973 2.pdf\n",
      "../resumes\\20.00973 3.pdf\n",
      "../resumes\\20.00973 4.pdf\n",
      "../resumes\\20.00973 5.pdf\n",
      "../resumes\\20.00973 6.pdf\n",
      "../resumes\\20.00973 7.pdf\n",
      "../resumes\\20.00973 8.pdf\n",
      "../resumes\\20.00973 9.pdf\n",
      "../resumes\\20.01139 0.pdf\n",
      "../resumes\\20.01139 1.pdf\n",
      "../resumes\\20.01139 2.pdf\n",
      "../resumes\\20.01139 3.pdf\n",
      "../resumes\\20.01139 4.pdf\n",
      "../resumes\\20.01139 5.pdf\n",
      "../resumes\\20.01139 6.pdf\n",
      "../resumes\\20.01139 7.pdf\n",
      "../resumes\\20.02068 0.pdf\n",
      "../resumes\\20.02068 1.pdf\n",
      "../resumes\\20.02068 2.pdf\n",
      "../resumes\\20.02068 3.pdf\n",
      "../resumes\\20.02068 4.pdf\n",
      "../resumes\\20.02068 5.pdf\n",
      "../resumes\\20.02068 6.pdf\n",
      "../resumes\\20.02068 7.pdf\n",
      "../resumes\\20.02564 0.pdf\n",
      "../resumes\\20.02564 1.pdf\n",
      "../resumes\\20.02564 10.pdf\n",
      "../resumes\\20.02564 11.pdf\n",
      "../resumes\\20.02564 12.pdf\n",
      "../resumes\\20.02564 2.pdf\n",
      "../resumes\\20.02564 3.pdf\n",
      "../resumes\\20.02564 4.pdf\n",
      "../resumes\\20.02564 5.pdf\n",
      "../resumes\\20.02564 6.pdf\n",
      "../resumes\\20.02564 7.pdf\n",
      "../resumes\\20.02564 8.pdf\n",
      "../resumes\\20.02564 9.pdf\n",
      "../resumes\\21.00599 0.pdf\n",
      "../resumes\\21.00599 1.pdf\n",
      "../resumes\\21.00599 10.pdf\n",
      "../resumes\\21.00599 11.pdf\n",
      "../resumes\\21.00599 12.pdf\n",
      "../resumes\\21.00599 13.pdf\n",
      "../resumes\\21.00599 2.pdf\n",
      "../resumes\\21.00599 3.pdf\n",
      "../resumes\\21.00599 4.pdf\n",
      "../resumes\\21.00599 5.pdf\n",
      "../resumes\\21.00599 6.pdf\n",
      "../resumes\\21.00599 7.pdf\n",
      "../resumes\\21.00599 8.pdf\n",
      "../resumes\\21.00599 9.pdf\n"
     ]
    }
   ],
   "source": [
    "# All the pdf files in the folder\n",
    "pdf_files = glob.glob(\"%s/*.pdf\" % \"../resumes/\")\n",
    "\n",
    "\n",
    "# Resumes -> convert to a dictionary with an employee name\n",
    "resumes = pd.DataFrame()\n",
    "\n",
    "texts = []\n",
    "# File loop\n",
    "for file in pdf_files:\n",
    "    print(file)\n",
    "    text = \"\"\n",
    "    try:\n",
    "        fileReader = PyPDF2.PdfFileReader(open(file,'rb'))\n",
    "        count = fileReader.numPages\n",
    "        text = \"\"\n",
    "        i = 0\n",
    "        while i < count:\n",
    "            text += fileReader.getPage(i).extractText()\n",
    "            i = i +1  \n",
    "        # lowercase\n",
    "        text = text.lower()\n",
    "        # remove nubmers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        # remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # remove \\n\n",
    "        text = text.replace('\\n',' ')\n",
    "        # remove ndł\n",
    "        text= text.replace('ndł',' ')\n",
    "        # remove \\nł\n",
    "        text = text.replace('\\nł',' ')\n",
    "        # remove ł\n",
    "        text = text.replace('ł',' ')\n",
    "        # remove \\nł\n",
    "        text = text.replace('\\nł',' ')\n",
    "\n",
    "        texts.append(text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    \n",
    "resumes['text'] = texts\n",
    "resumes['classification'] = \"Resume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes = resumes.append(resumesTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text into a vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "\n",
    "# Do not call fit transform twice because it is stored in vectorizer \n",
    "X_train = vectorizer.fit_transform(resumes['text'])\n",
    "X_test = vectorizer.transform(resumesTest['text'])\n",
    "y_train = resumes['classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)  \n",
    "text_clf  = classifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../downloaded/20.02373 CDOT NPS Western Slope_Ulteig.pdf'\n",
    "fileReader = PyPDF2.PdfFileReader(open(file,'rb'))\n",
    "count = fileReader.numPages\n",
    "\n",
    "text = \"\"\n",
    "i = 0\n",
    "X_test = []\n",
    "while i < count:\n",
    "    text = fileReader.getPage(i).extractText()\n",
    "    X_test.append(text)\n",
    "    \n",
    "    i = i +1  \n",
    "\n",
    "#X_test = vectorizer.transform(X_test)\n",
    "#text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Statement of Interest'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['NotResume', 'Resume', 'NotResume'], dtype=object),\n",
       " <3x14887 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 16932 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict(X_test), X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
